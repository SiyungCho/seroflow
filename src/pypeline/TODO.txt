TODO:

v3
- fix adl e/l
- add chunking
    - Batching data
    - intermediate checkpoints
        - probably do combination of both, so when you batch data, you can add a checkpoint step to chunk up to a certain point then process normally or continue batching?
- add pre-made custom steps
    - add/delete dataframe
    - add/delete/update internal variable
    - perform sql query on dataframe or entire context (ie all dataframes)
    - convert dataframe column type
    - change variable name
    - change dataframe name/properties
    - template for adding custom context/dataframes
    - transpose dataframe
    - get mean/median/mode of dataframe column and store in variable

v4
- fix logger and turn logger off and on
- add universal engine
- implement different 'modes' for pypeline (and maybe for steps)
- implement 'on_fail' functionality

future
- add MT and MP
- add REST compatible e/l
- use other container types (numpy, spark dataframes)