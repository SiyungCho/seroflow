TODO:

v3
- fix adl e/l
- add chunking
    - Batching data
    - intermediate checkpoints
        - probably do combination of both, so when you batch data, you can add a checkpoint step to chunk up to a certain point then process normally or continue batching?
        - try using other libraries as per the pandas documentation 
- add pre-made custom steps
    - add/delete dataframe
    - add/delete/update internal variable
    - perform sql query on dataframe or entire context (ie all dataframes)
    - convert dataframe column type
    - change variable name
    - change dataframe name/properties
    - template for adding custom context/dataframes
    - transpose dataframe
    - get mean/median/mode of dataframe column and store in variable
- look into adding other pandas compatible APIs (https://pandas.pydata.org/community/ecosystem.html#out-of-core)

v4
- fix logger and turn logger off and on
- add universal engine
- implement different 'modes' for pypeline (and maybe for steps) 
    - (in dev mode, executes all steps but doesnot actually release exports detailed review of global context) 
    - (in test mode: executes the entire pypeline, but only on random sampled values, exports detailed review of global context and detailed review of execution, but only for random sampled values) 
    - (in prod mode, executes entire pypeline on whole dataset, generates detailed review of execution on all data)
- implement 'on_fail' functionality (ignore, log, raise)

future
- add MT and MP
- add REST compatible e/l
- use other container types (numpy, spark dataframes)
- optimized data viz

Chunking implementation:
we are going to implement 2 main principles behind chunking data
1. Recursive Chunking
    - at each extractor
        - number of chunks is determined by the floor(largest df/chunksize)
        - # rows is determined proportionally to amount of chunks (spread evenly)
    - multiple extractors
        - previous chunks and new data rows are spread evenly across new chunks downstream
2. Direct Chunking
    - at each extractor
        - number of chunks is determined by the floor(largest df/chunksize)
        - # rows is equal to chunk size, once smaller dfs reach end then empty df is used in their place for subsequent chunks
    - multiple extractors
        - directly gather data one after another by chunksize
